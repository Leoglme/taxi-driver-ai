import numpy as np
import gymnasium as gym
import random
from gymnasium.spaces import Discrete

# Cr√©ation de l'environnement Taxi
env = gym.make("Taxi-v3", render_mode=None)

# V√©rification que les espaces sont de type Discrete
assert isinstance(env.observation_space, Discrete), "L'espace d'observation doit √™tre Discrete"
assert isinstance(env.action_space, Discrete), "L'espace d'action doit √™tre Discrete"

# Param√®tres du Q-learning
learning_rate = 0.1  # Taux d'apprentissage
discount_factor = 0.99  # Importance des r√©compenses futures
epsilon = 1.0  # Taux initial d'exploration
epsilon_decay = 0.999  # D√©croissance d'epsilon apr√®s chaque √©pisode
min_epsilon = 0.01  # Valeur minimale d'epsilon
num_episodes = 50000  # Nombre d'√©pisodes d'entra√Ænement
max_steps = 200  # Nombre maximal d'actions par √©pisode (limite de l'environnement Taxi)

# Initialisation de la Q-table (500 √©tats x 6 actions)
n_states = env.observation_space.n  # 500
n_actions = env.action_space.n  # 6
Q_table = np.zeros((n_states, n_actions))

# Entra√Ænement de l'agent
for episode in range(num_episodes):
    state, _ = env.reset()

    for step in range(max_steps):
        # S√©lection d'une action avec la strat√©gie epsilon-greedy
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # Exploration
        else:
            action = np.argmax(Q_table[state])  # Exploitation

        new_state, reward, terminated, truncated, info = env.step(action)

        # La r√©compense de Taxi est d√©j√† d√©finie :
        # -1 par pas, +20 pour un d√©p√¥t r√©ussi, -10 pour une action ill√©gale.
        # (On peut √©ventuellement appliquer du reward shaping suppl√©mentaire ici.)
        td_target = reward + discount_factor * np.max(Q_table[new_state])
        td_error = td_target - Q_table[state, action]
        Q_table[state, action] += learning_rate * td_error

        state = new_state

        if terminated or truncated:
            break

    # D√©croissance progressive d'epsilon pour r√©duire l'exploration au fil de l'entra√Ænement
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    if episode % 1000 == 0:
        print(f"√âpisode {episode}, Epsilon: {epsilon:.3f}")

print("Entra√Ænement termin√© !")

# Phase de test (mode "human" pour visualiser)
env = gym.make("Taxi-v3", render_mode="human")
num_test_episodes = 10

for episode in range(num_test_episodes):
    state, _ = env.reset()
    step = 0
    print(f"\nüîπ Test {episode + 1}")

    while True:
        env.render()
        action = np.argmax(Q_table[state])
        new_state, reward, terminated, truncated, info = env.step(action)
        state = new_state
        step += 1

        if terminated or truncated:
            # En cas de d√©p√¥t r√©ussi, l'environnement retourne la r√©compense +20
            if reward == 20:
                print(f"üèÜ Succ√®s : Passager d√©pos√© avec succ√®s en {step} √©tapes !")
            else:
                print(f"üíÄ √âchec apr√®s {step} √©tapes...")
            break

env.close()