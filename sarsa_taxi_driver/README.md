# ğŸš– SARSA Agent pour Taxi-v3 (Gymnasium)

Ce projet implÃ©mente un agent utilisant lâ€™algorithme SARSA (State-Action-Reward-State-Action) pour apprendre Ã  rÃ©soudre lâ€™environnement Taxi-v3 de Gymnasium. Lâ€™objectif est que lâ€™agent apprenne Ã  transporter un passager Ã  sa destination de maniÃ¨re efficace en maximisant la rÃ©compense.

## ğŸ§  Algorithme

SARSA est un algorithme dâ€™apprentissage par renforcement **on-policy**. Il met Ã  jour sa Q-table en fonction de la sÃ©quence (Ã©tat, action, rÃ©compense, nouvel Ã©tat, nouvelle action). Contrairement Ã  Q-learning (off-policy), SARSA prend en compte la politique actuelle de lâ€™agent pour choisir la prochaine action.

**Formule de mise Ã  jour :**  
Q(s, a) â† Q(s, a) + Î± [r + Î³ * Q(s', a') - Q(s, a)]  
oÃ¹ :
- `Î±` : taux dâ€™apprentissage (learning rate)
- `Î³` : facteur de discount (discount factor)
- `s` : Ã©tat courant
- `a` : action courante
- `r` : rÃ©compense obtenue
- `s'` : Ã©tat suivant
- `a'` : action suivante choisie

## âš™ï¸ ParamÃ¨tres d'entraÃ®nement

- Environnement : `Taxi-v3` (500 Ã©tats, 6 actions)
- StratÃ©gie d'exploration : epsilon-greedy
- `learning_rate = 0.1`
- `discount_factor = 0.99`
- `epsilon = 1.0` avec `epsilon_decay = 0.999`
- `min_epsilon = 0.01`
- `episodes = 50 000`
- `max_steps = 200` par Ã©pisode

## ğŸ“Š Analyse de l'entraÃ®nement

- Lâ€™agent commence en explorant largement grÃ¢ce Ã  un epsilon Ã©levÃ©.
- GrÃ¢ce Ã  lâ€™epsilon decay, il apprend progressivement Ã  **exploiter les actions les plus rentables**.
- Vers la fin de lâ€™apprentissage, epsilon est proche de 0.01, ce qui signifie que lâ€™agent **agit presque toujours selon ce quâ€™il a appris**.
- Lors des tests, lâ€™agent atteint sa destination dans la majoritÃ© des cas en un nombre rÃ©duit dâ€™Ã©tapes.
- Cela confirme que la **Q-table a convergÃ© vers une politique efficace**.
- Le choix dâ€™un `discount_factor` Ã©levÃ© (0.99) permet Ã  lâ€™agent de planifier Ã  long terme et maximiser la rÃ©compense finale (+20).

## ğŸ RÃ©sultats

- Lâ€™agent rÃ©ussit de plus en plus de trajets vers la fin de lâ€™entraÃ®nement.
- Epsilon dÃ©croÃ®t au fil des Ã©pisodes, ce qui permet de passer de lâ€™exploration Ã  lâ€™exploitation.
- Le test final avec rendu graphique montre que lâ€™agent accomplit la mission dans la plupart des cas.

## â–¶ï¸ ExÃ©cution

Assurez-vous d'avoir installÃ© les dÃ©pendances :

```bash
pip install gymnasium numpy
```

Puis lancez le script principal :

```bash
python sarsa_taxi.py
```

Ã€ la fin de l'entraÃ®nement, 10 Ã©pisodes de test sont jouÃ©s en mode `render_mode="human"` pour visualiser les performances de lâ€™agent.

## ğŸ”­ Pistes d'amÃ©lioration

- Comparaison avec Q-learning pour analyser les diffÃ©rences de performance.
- Ajout dâ€™un suivi graphique des rÃ©compenses par Ã©pisode.
- Application Ã  des environnements plus complexes avec Q-networks (DQN).

## ğŸ“ Fichiers

- `sarsa_taxi.py` : Script principal avec entraÃ®nement et test de lâ€™agent SARSA
- `README.md` : PrÃ©sentation du projet

---
