# ğŸ§  Q-Learning Agent pour Taxi-v3 ğŸš•  

Ce module implÃ©mente un **agent basÃ© sur lâ€™algorithme de Q-Learning** pour rÃ©soudre lâ€™environnement `Taxi-v3` de Gymnasium.

## ğŸ“š Description de lâ€™algorithme

Le Q-Learning est un algorithme **model-free off-policy** dâ€™apprentissage par renforcement. Il permet Ã  un agent dâ€™apprendre Ã  partir de son environnement en mettant Ã  jour une **Q-table** (table des valeurs dâ€™action) Ã  chaque interaction.

Le Q-Learning apprend au fur et Ã  mesure, Ã©tape par Ã©tape.
Ã€ chaque action, il met Ã  jour la Q-table immÃ©diatement en estimant la meilleure rÃ©compense future possible.


### ğŸ” Logique de mise Ã  jour :
Ã€ chaque Ã©tape, on met Ã  jour la Q-table selon la formule :

```

Q(s, a) â† Q(s, a) + Î± Ã— \[r + Î³ Ã— max(Q(sâ€², aâ€²)) âˆ’ Q(s, a)]

```

- `s` : Ã©tat courant  
- `a` : action choisie  
- `r` : rÃ©compense reÃ§ue  
- `sâ€²` : nouvel Ã©tat  
- `Î±` : taux dâ€™apprentissage (learning rate)  
- `Î³` : facteur de discount  
- `max(Q(sâ€², aâ€²))` : meilleure action possible dans le nouvel Ã©tat

---

## âš™ï¸ ParamÃ¨tres principaux

| ParamÃ¨tre         | Valeur par dÃ©faut | Description                                      |
|-------------------|-------------------|--------------------------------------------------|
| `learning_rate`   | `0.1`             | Vitesse dâ€™apprentissage de lâ€™agent               |
| `discount_factor` | `0.99`            | PondÃ©ration des rÃ©compenses futures              |
| `epsilon`         | `1.0`             | ProbabilitÃ© de choisir une action alÃ©atoire      |
| `epsilon_decay`   | `0.999`           | RÃ©duction progressive de lâ€™exploration           |
| `min_epsilon`     | `0.01`            | Valeur minimale dâ€™epsilon                        |
| `num_episodes`    | `50000`           | Nombre dâ€™Ã©pisodes pour lâ€™entraÃ®nement            |
| `max_steps`       | `200`             | Nombre maximal dâ€™actions par Ã©pisode             |

---

## ğŸ§ª Phases de lâ€™entraÃ®nement

- **Exploration** : au dÃ©but, lâ€™agent explore lâ€™environnement avec un taux Ã©levÃ© dâ€™epsilon.
- **Exploitation** : au fur et Ã  mesure que lâ€™agent apprend, `epsilon` diminue, ce qui favorise les actions les plus prometteuses.
- **Stabilisation** : une fois que `epsilon` atteint son minimum, lâ€™agent applique une politique quasi-dÃ©terministe basÃ©e sur la Q-table.

---

## âœ… RÃ©sultats attendus

- Un agent **naÃ¯f (Îµ=1)** peut nÃ©cessiter jusquâ€™Ã  **300â€“350 Ã©tapes** pour terminer un Ã©pisode.
- Un agent **entraÃ®nÃ©** (avec epsilon rÃ©duit et Q-table bien remplie) peut atteindre des **performances optimales autour de 20 Ã©tapes** pour rÃ©ussir.

---

## ğŸ“ˆ Suivi de l'apprentissage

Un log est affichÃ© tous les 1000 Ã©pisodes :

```

\[Q-Learning] Ã‰pisode 3000, Îµ=0.406

````

Cela permet de suivre la **dÃ©croissance de l'exploration** et de diagnostiquer un Ã©ventuel blocage.

---

## ğŸ“‚ Fichiers

- `q_learning_agent.py` : contient la classe `QLearningAgent` avec toute la logique.
- UtilisÃ© dans `main.py` pour lancer l'entraÃ®nement ou les tests.

---

## ğŸ’¬ Exemple dâ€™utilisation

```bash
$ python main.py
=== Choix de l'algorithme ===
1) Q-learning
2) Monte Carlo
Entrez 1 ou 2 puis [EntrÃ©e] : 1
````

Puis lâ€™agent sâ€™entraÃ®ne pendant 50 000 Ã©pisodes avant dâ€™Ãªtre testÃ© dans 10 parties en mode visuel.

---

## ğŸ” Analyse rapide

âœ… **Points forts** :

* Simple, rapide Ã  implÃ©menter.
* TrÃ¨s efficace dans des environnements discrets comme Taxi-v3.

âš ï¸ **Limites** :

* Ne fonctionne pas bien si lâ€™espace dâ€™Ã©tats ou dâ€™actions est trop grand.
* NÃ©cessite de stocker une Q-table complÃ¨te (ici 500Ã—6 = 3000 valeurs).

