# ğŸ² Monte Carlo Agent pour Taxi-v3 ğŸš•

Ce module contient une implÃ©mentation dâ€™un **agent Monte Carlo** pour rÃ©soudre lâ€™environnement `Taxi-v3` de Gymnasium.

## ğŸ“š Description de lâ€™algorithme

Lâ€™algorithme Monte Carlo est une mÃ©thode **model-free** dâ€™apprentissage par renforcement, qui se base sur lâ€™expÃ©rience **complÃ¨te dâ€™un Ã©pisode** (du dÃ©but Ã  la fin) pour mettre Ã  jour les estimations des valeurs dâ€™action (**Q-values**).

Monte Carlo apprend Ã  la fin de lâ€™Ã©pisode.
Il regarde tout ce quâ€™il sâ€™est passÃ© et calcule la rÃ©compense totale reÃ§ue pour chaque action prise.

Contrairement au Q-Learning (qui met Ã  jour Ã  chaque Ã©tape), Monte Carlo :
- **observe la totalitÃ© de lâ€™Ã©pisode**,
- **calcule le retour total `G`** pour chaque (Ã©tat, action),
- puis **met Ã  jour la Q-table** uniquement Ã  la premiÃ¨re visite de chaque paire (Ã©tat, action).

---

## ğŸ” Mise Ã  jour Monte Carlo (First-Visit)

Ã€ la fin dâ€™un Ã©pisode, pour chaque `(Ã©tat, action)` rencontrÃ© pour la premiÃ¨re fois dans lâ€™Ã©pisode :

```

G = râ‚œ + Î³ \* râ‚œâ‚Šâ‚ + Î³Â² \* râ‚œâ‚Šâ‚‚ + ...
Q(s, a) â† Moyenne de tous les retours G observÃ©s pour (s, a)

````

Cela permet une **estimation stable**, mais nÃ©cessite dâ€™attendre la fin de chaque Ã©pisode pour apprendre.

---

## âš™ï¸ ParamÃ¨tres principaux

| ParamÃ¨tre         | Valeur par dÃ©faut | Description                                      |
|-------------------|-------------------|--------------------------------------------------|
| `discount_factor` | `0.99`            | PondÃ©ration des rÃ©compenses futures              |
| `epsilon`         | `1.0`             | Taux dâ€™exploration initial                       |
| `epsilon_decay`   | `0.999`           | RÃ©duction progressive de lâ€™exploration           |
| `min_epsilon`     | `0.01`            | Valeur minimale dâ€™epsilon                        |
| `num_episodes`    | `50000`           | Nombre dâ€™Ã©pisodes dâ€™apprentissage                |
| `max_steps`       | `200`             | Nombre max dâ€™actions par Ã©pisode                 |

---

## ğŸ“‚ Composants du code

- `generate_episode(env, max_steps)` : gÃ©nÃ¨re une trajectoire complÃ¨te de lâ€™agent (Ã©tats, actions, rÃ©compenses).
- `update(episode)` : met Ã  jour les Q-values Ã  partir du retour total `G`, en appliquant la mÃ©thode **First-Visit Monte Carlo**.
- `choose_action(state)` : applique une politique **epsilon-greedy** pour Ã©quilibrer exploration et exploitation.

---

## ğŸ§  Avantages et limites

âœ… **Forces** :
- Simple Ã  implÃ©menter.
- Fournit des estimations stables sur le long terme.
- Ne nÃ©cessite pas de connaissance du modÃ¨le de lâ€™environnement.

âš ï¸ **Limites** :
- NÃ©cessite **la fin de lâ€™Ã©pisode** pour apprendre.
- Moins efficace dans des environnements avec **longs Ã©pisodes ou grande variance**.
- Moins rapide que le Q-learning pour converger dans certains cas.

---

## âœ… Objectif du projet

Ce Monte Carlo agent est destinÃ© Ã  Ãªtre comparÃ© Ã  dâ€™autres approches, comme :

- **Q-Learning** (dÃ©jÃ  implÃ©mentÃ©)
- **Deep Q-Learning** (Ã  venir)
- **Brute Force** (baseline naÃ¯ve)

Lâ€™objectif est dâ€™Ã©valuer leurs performances (vitesse de convergence, nombre dâ€™Ã©tapes, rÃ©compenses moyennes...) sur un mÃªme environnement.

---

## ğŸ“ˆ Exemple dâ€™affichage

```bash
[MonteCarlo] Ã‰pisode 3000, Îµ=0.421
````

---

## ğŸ§ª Utilisation

Depuis `main.py` :

```bash
$ python main.py
=== Choix de l'algorithme ===
1) Q-learning
2) Monte Carlo
Entrez 1 ou 2 puis [EntrÃ©e] : 2
```

Lâ€™agent sâ€™entraÃ®ne puis est testÃ© visuellement sur 10 Ã©pisodes consÃ©cutifs.
